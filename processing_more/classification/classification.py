# -*- coding: utf-8 -*-
"""21z205.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSfEUSbs13gshCx-Y3GnQlDTeZrcpaJ2

## Importing Required Libaries
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import Normalizer, OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from datetime import datetime
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler

"""REading CSV File"""

dat = pd.read_csv('weatherAUS.csv')

"""# Converting date columns"""

def crt_dat(dt):
    tem= datetime.strptime(dt, "%Y-%m-%d")
    epoch = datetime(2008, 1, 1)
    days_from_epoch = (tem - epoch).days
    return days_from_epoch

# print(dat['region'].isna().any())
dat['Date']= dat['Date'].apply(crt_dat)

unique_values_B = dat['RainTomorrow'].unique()
print("Unique values in column 'B':", unique_values_B)

"""# Label encoding (Yes No Colums)"""

dat = dat.dropna(subset=['RainToday'])
dat = dat.dropna(subset=['RainTomorrow'])

print(dat['RainToday'].isna().sum())
print(dat['RainTomorrow'].isna().sum())


label_encoder = LabelEncoder()

# Fit label encoder and transform the specified column
dat['RainToday'] = label_encoder.fit_transform(dat['RainToday'])
dat['RainTomorrow'] = label_encoder.fit_transform(dat['RainTomorrow'])

dat.head(10)

unique_values_B = dat['RainTomorrow'].unique()
print("Unique values in column 'B':", unique_values_B)

print(dat['Evaporation'].isna().sum())
dat.drop(columns=['Evaporation', 'Sunshine'], inplace=True)

"""# Handling Nan Values

Numerical Values
"""

numeric_columns = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am',
                   'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am',
                   'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm',
                   'RainToday', 'RainTomorrow']

for location in dat['Location'].unique():
    # Filter the DataFrame based on the current location
    condition = (dat['Location'] == location)
    filtered_data = dat[condition].copy()

    # Exclude columns with all NaN values
    numeric_columns_filtered = filtered_data[numeric_columns].columns[filtered_data[numeric_columns].notnull().any()]

    # Convert columns to numeric
    filtered_data[numeric_columns_filtered] = filtered_data[numeric_columns_filtered].apply(pd.to_numeric, errors='coerce')

    # Initialize and fit the imputer
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    imputer.fit(filtered_data[numeric_columns_filtered])

    # Transform the values
    transformed_values = imputer.transform(filtered_data[numeric_columns_filtered])

    # Assign values back to the DataFrame
    dat.loc[condition, numeric_columns_filtered] = transformed_values

columns_to_fill = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am',
                   'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am',
                   'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm',
                   'RainToday', 'RainTomorrow']


dat = dat.fillna(dat.mean())

# 'WindGustDir', 'WindDir9am''WindDir3pm',
for i in numeric_columns:
   print(i,dat[i].isna().sum())



dat.head(10)

unique_values_B = dat['RainTomorrow'].unique()
print("Unique values in column 'B':", unique_values_B)

"""String Values"""

str_col =['WindDir3pm', 'WindDir9am', 'WindGustDir',]
for i in str_col:
  dat = dat.dropna(subset=[i])

numeric_columns.append("Date")
numeric_columns.remove("RainTomorrow")
numeric_columns.remove("RainToday")

"""# Removing Outliers"""

def remove_outliers_iqr(data):
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1
    lower_bound = Q1 -( 1.5 * IQR ) # Adjusted multiplier to 1.5 for typical IQR outlier rule
    upper_bound = Q3 + (1.5 * IQR  )# Adjusted multiplier to 1.5 for typical IQR outlier rule
    def rm(dt):
        if (dt < lower_bound) or (dt > upper_bound):
            return np.nan
        return dt
    return data.apply(rm)

# Function to remove outliers using z-score method
def remove_outliers_zscore(data):
    numeric_data = data.select_dtypes(include=[np.number])  # Select only numeric columns
    z_scores = zscore(numeric_data)
    abs_z_scores = np.abs(z_scores)
    filtered_entries = (abs_z_scores < 3).all(axis=1)  # Adjust the threshold as needed
    return data[filtered_entries]

# dat = remove_outliers_zscore(dat)

for i in numeric_columns:
    dat[i] = remove_outliers_iqr(dat[i])
    # dat = dat.dropna(subset=[i])
dat = dat.dropna()
# for i in columns_to_normalize_tw:
#     dat[i] = remove_outliers_iqr(dat[i])
#     dat = dat.dropna(subset=[i])

# for j in range(0,15):
#     for i in columns_to_normalize_tw:
#         dat[i] = remove_outliers_iqr(dat[i])
#         dat = dat.dropna(subset=[i])
# dat.to_csv('avooutl.csv')
dat.reset_index(drop=True, inplace=True)

"""# Normalising Values"""

dat[numeric_columns] = dat[numeric_columns].fillna(0)

# Extract the numeric columns
tem = dat[numeric_columns]

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler()
tem_normalized = scaler.fit_transform(tem)

# Create a DataFrame from normalized data with the same columns as the original DataFrame
normalized_df = pd.DataFrame(tem_normalized, columns=numeric_columns)

# Update the original DataFrame with normalized values
dat[numeric_columns] = normalized_df[numeric_columns]

print(dat.head(20))

dat.head()

for column in dat.columns:
    na_count = dat[column].isna().sum()
    print(f"Column '{column}': {na_count} NaN values")

"""# Onehot Encoding"""

cols_onehot = ['WindDir3pm', 'WindDir9am', 'WindGustDir', 'Location']



ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cols_onehot)], remainder='passthrough')

# Fit and transform the data
transformed_data = ct.fit_transform(dat)
transformed_data_dense = transformed_data.toarray()

# Convert the dense array to a DataFrame
dat = pd.DataFrame(transformed_data_dense, columns=ct.get_feature_names_out())

dat.head()

"""# Writing Processed CSV"""

dat.to_csv(r'weather_processed.csv')

"""# Importing required packages"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB

"""# Spliting dataset"""

# Split the data into training and testing sets
X = dat.drop(columns = ['remainder__RainTomorrow'])
y = dat['remainder__RainTomorrow']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Fiting Logistic Regression model and Naïve Bayes classifie"""

# Fit Logistic Regression model
log_reg = LogisticRegression(max_iter = 1500, solver = 'liblinear')
log_reg.fit(X_train, y_train)

# Fit Naïve Bayes classifier
naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

"""# Construct confusion matrix for Logistic Regression and Naive Bayes classifier"""

# Predictions
log_reg_preds = log_reg.predict(X_test)
naive_bayes_preds = naive_bayes.predict(X_test)

# Confusion Matrix
log_reg_cm = confusion_matrix(y_test, log_reg_preds)
naive_bayes_cm = confusion_matrix(y_test, naive_bayes_preds)

# Plot confusion matrix with seaborn heatmap
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
sns.heatmap(log_reg_cm, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

plt.subplot(1, 2, 2)
sns.heatmap(naive_bayes_cm, annot=True, cmap='Greens', fmt='d', cbar=False)
plt.title('Naïve Bayes Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

plt.tight_layout()
plt.show()

"""# Classification Report"""

# Classification Report
log_reg_report = classification_report(y_test, log_reg_preds)
naive_bayes_report = classification_report(y_test, naive_bayes_preds)

print("Logistic Regression Classification Report:")
print(log_reg_report)

print("\nNaïve Bayes Classification Report:")
print(naive_bayes_report)

"""#  Computing ROC curve and ROC-AUC score"""

log_reg_probs = log_reg.predict_proba(X_test)[:, 1]
naive_bayes_probs = naive_bayes.predict_proba(X_test)[:, 1]

# Compute ROC curve and ROC-AUC score for Logistic Regression
log_reg_fpr, log_reg_tpr, _ = roc_curve(y_test, log_reg_probs)
log_reg_auc = roc_auc_score(y_test, log_reg_probs)

# Compute ROC curve and ROC-AUC score for Naïve Bayes
naive_bayes_fpr, naive_bayes_tpr, _ = roc_curve(y_test, naive_bayes_probs)
naive_bayes_auc = roc_auc_score(y_test, naive_bayes_probs)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(log_reg_fpr, log_reg_tpr, label=f'Logistic Regression (AUC = {log_reg_auc:.2f})')
plt.plot(naive_bayes_fpr, naive_bayes_tpr, label=f'Naïve Bayes (AUC = {naive_bayes_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""# Performing Cross validation"""

def perform_cross_validation(model, X, y, cv=3):
    scores = cross_val_score(model, X, y, cv=cv)
    print(f"Cross-Validation Scores (Accuracy) - {cv}-Fold Cross-Validation:")
    print(scores)
    print(f"Mean Accuracy: {np.mean(scores):.2f}")
    print(f"Standard Deviation: {np.std(scores):.2f}")

perform_cross_validation(log_reg, X, y)
perform_cross_validation(naive_bayes, X, y)

"""# Hyperparameter Tuning"""

# Define hyperparameters grid
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

# Perform GridSearchCV
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
log_best_model = grid_search.best_estimator_

# Perform cross-validation with the best model
perform_cross_validation(log_best_model, X_train, y_train)

# Define hyperparameters to tune
param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5,1e-4,1e-3,1e-2,0.1,1,10,100]}

# Define grid search with cross-validation
grid_search = GridSearchCV(estimator=naive_bayes, param_grid=param_grid, cv=5, scoring='accuracy')

# Perform grid search
grid_search.fit(X, y)

# Get best hyperparameters and performance
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best hyperparameters:", best_params)
print("Best accuracy:", best_score)

# Get the best model
nb_best_model = grid_search.best_estimator_

# Perform cross-validation with the best model
perform_cross_validation(nb_best_model, X_train, y_train)

"""# Confusion Matrix afer hyperparameter tuning"""

# Predictions
log_reg_preds = log_best_model.predict(X_test)
naive_bayes_preds = nb_best_model.predict(X_test)

# Confusion Matrix
log_reg_cm = confusion_matrix(y_test, log_reg_preds)
naive_bayes_cm = confusion_matrix(y_test, naive_bayes_preds)

# Plot confusion matrix with seaborn heatmap
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
sns.heatmap(log_reg_cm, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

plt.subplot(1, 2, 2)
sns.heatmap(naive_bayes_cm, annot=True, cmap='Greens', fmt='d', cbar=False)
plt.title('Naïve Bayes Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

plt.tight_layout()
plt.show()

"""# Inference

Intialy there are lot of unecesary fileds like Evaporation,Sunshine which we find on further analysing and these coloums are remove as they have a lot of nan values. The  on further cleaning and fiting the model we find an Cross-Validation Score of 78% and 66%. Then after performing hyperparameter tuning using various values we find that the Cross-Validation Score has been increaseed to 87% and 85%.

# Conclusion

We can conclude that removing irrevelant data colums and performing hyperparmeter tuning can help to greatly enhance accuracy.
"""